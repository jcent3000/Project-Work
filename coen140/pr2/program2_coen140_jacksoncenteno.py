# -*- coding: utf-8 -*-
"""program2_coen140_JacksonCenteno.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mgQr9428Xqz98qY3aJjwQcU8bYoIlWGl
"""

from sklearn import preprocessing
import pandas as pd
import scipy.sparse as sp
from numpy.linalg import norm
from collections import Counter, defaultdict
from scipy.sparse import csr_matrix
import numpy as np


df_tr = pd.read_csv(
        'train.dat', 
        header=None,
        delimiter='\t',
        names = ["class", "peptide"]
)
df_te = pd.read_csv(
        'test.dat', 
        header=None,
        delimiter='\t',
        names = ["peptide"]
)

# Extract features and labels

x_train = df_tr.iloc[:,1].values
y_train = df_tr.iloc[:,0].values
x_test = df_te.iloc[:,:].values.ravel()

print(y_train.shape)

def cmer(name, c=3):
    r""" Given a name and parameter c, return the vector of c-mers associated with the name
    """
    if len(name) < c:
        return [name]
    v = []
    for i in range(len(name)-c+1):
        v.append(name[i:(i+c)])
    return v

def build_matrix(docs, idx = {}):
    r""" Build sparse matrix from a list of documents, 
    each of which is a list of word/terms in the document.  
    """
    nrows = len(docs)
    tid = 0
    nnz = 0
    if idx:
      for d in docs:
        nnz += len([w for w in set(d) if w in idx])
    else:
      for d in docs:
          nnz += len(set(d))
          for w in d:
              if w not in idx:
                  idx[w] = tid
                  tid += 1
    ncols = len(idx)
        
    # set up memory
    ind = np.zeros(nnz, dtype=np.int)
    val = np.zeros(nnz, dtype=np.double)
    ptr = np.zeros(nrows+1, dtype=np.int)
    i = 0  # document ID / row counter
    n = 0  # non-zero counter
    # transfer values
    for d in docs:
        cnt = Counter(d)
        keys = list(k for k,_ in cnt.most_common() if k in idx)
        l = len(keys)
        for j,k in enumerate(keys):
            ind[j+n] = idx[k]
            val[j+n] = cnt[k]
        ptr[i+1] = ptr[i] + l
        n += l
        i += 1
            
    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)
    mat.sort_indices()
    return mat, idx

def csr_l2normalize(mat, copy=False, **kargs):
    r""" Normalize the rows of a CSR matrix by their L-2 norm. 
    If copy is True, returns a copy of the normalized matrix.
    """
    if copy is True:
        mat = mat.copy()
    nrows = mat.shape[0]
    nnz = mat.nnz
    ind, val, ptr = mat.indices, mat.data, mat.indptr
    # normalize
    for i in range(nrows):
        rsum = 0.0    
        for j in range(ptr[i], ptr[i+1]):
            rsum += val[j]**2
        if rsum == 0.0:
            continue  # do not normalize empty rows
        rsum = 1.0/np.sqrt(rsum)
        for j in range(ptr[i], ptr[i+1]):
            val[j] *= rsum
            
    if copy is True:
        return mat
        
def namesToMatrix(names, c):
    docs = [cmer(n[0], c) for n in names]
    return build_matrix(docs)

idx_ = {}

mat_train, idx_ = build_matrix([cmer(n,2) for n in x_train])
csr_l2normalize(mat_train)
print(mat_train.get_shape)


mat_test, idx2 = build_matrix([cmer(n,2) for n in x_test], idx_)
csr_l2normalize(mat_test)
print(mat_test.get_shape)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from numpy import mean
from numpy import std
def evaluate_model(model, X, y):
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=100000000)
    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
    return scores

lda = LinearDiscriminantAnalysis()
scores = evaluate_model(lda, mat_train.toarray(), y_train)
print('Cross validation mean accuracy: {:.4f}'.format(scores.mean()))

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

xtrain, xtest, ytrain, ytest = train_test_split(
    mat_train, y_train, test_size=0.30, random_state=100000000)

lda = LinearDiscriminantAnalysis()
lda.fit(xtrain.toarray(), ytrain)
y_pred = lda.predict(xtest)
acc = accuracy_score(ytest, lda.predict(xtest))
print('Accuracy: {:.4f}'.format(acc))

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis


#xtrain = xtrain.toarray().ravel().reshape(-1,1)
#xtest = xtest.toarray().ravel().reshape(-1,1)

lda = LinearDiscriminantAnalysis()
lda.fit(mat_train.toarray(), y_train)
y_pred = lda.predict(mat_test.toarray())
print(y_pred)